{"metadata":{"language_info":{"name":"python","version":"3.7.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# fastAI book chapter 7 - summary","metadata":{}},{"cell_type":"markdown","source":"Chapter 7 comprises some techniques to optimize training","metadata":{}},{"cell_type":"markdown","source":"# Normalization","metadata":{}},{"cell_type":"markdown","source":"If transfer learning is used it's important to normalize the data using the same statistics as used for the dataset the initial training was based on.<br>\nThe statistics can be included in the definition of the datablock:","metadata":{}},{"cell_type":"code","source":"dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                   get_items=get_image_files,\n                   get_y=parent_label,\n                   item_tfms=Resize(460),\n                   batch_tfms=[*aug_transforms(size=size, min_scale=0.75),\n                               Normalize.from_stats(*imagenet_stats)])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Progressive resizing","metadata":{}},{"cell_type":"markdown","source":"Step by step increase the size of the images while training.<br>\nHowever, this is probably not going to work for transfer learning since the original training might have been performed using images with one specific size. ","metadata":{}},{"cell_type":"markdown","source":"# Test Time Validation","metadata":{}},{"cell_type":"markdown","source":"When random cropping is used for augmentation of training data, usually the validation data is also cropped. However, not random but in the center only. <br>\nTo avoid losing data and hence accuracy we can perform multiple crops ratered over the whole image and classify each of teh crops. Subsequently, the predictions can either be averaged or the max can be used as final pred for that iamge. ","metadata":{}},{"cell_type":"markdown","source":"# Mixup augmentation","metadata":{}},{"cell_type":"markdown","source":"Imagine a multilable problem with a one-hot labelling strategy. When using sigmoid or softmax the prediction is never going to be 1 or 0 but will try it's hardest to approach these. This can lead to overfitting. Mixup data augmentation deals with this issue automatically. <br>\n\nMixup computes a linear combination of two different items in the training dataset using random weights. The corresponding labels are also combined using the same weights yielding a one-hot tensor with two or more entries <=1.","metadata":{}},{"cell_type":"markdown","source":"Mixup prolongs training significantly but can increase accuracy. ","metadata":{}},{"cell_type":"markdown","source":"# Label smoothing","metadata":{}},{"cell_type":"markdown","source":"This is also a technique which adresses the problem described above. Here we substitue 0s in teh one-hot tensor with $\\frac{\\epsilon}{N}$ and 1s with $1-\\epsilon + \\frac{\\epsilon}{N}$ <br>\nThis way we don't have 0s and 1s but the tensor still sums up to 1.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}